{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4280167",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "- Built upon previous work [2023-06-30_cleaning_df_bothdata.py](https://github.com/tiangenglu/WebScrape/blob/main/06302023_cleaning_df_bothdata.py)\n",
    "- Detect new raw data and ONLY process the newly scraped data\n",
    "- After cleaning, append to the existing all-time visa data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cb37c",
   "metadata": {},
   "source": [
    "**NOTES**: The program needs minor revisions to read \"old\" data from S3 instead of local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22c7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ee15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retire this chunk\n",
    "# move aws s3 connection up\n",
    "# use niv_alltime and iv_alltime\n",
    "# old_data = pd.read_csv(\"visa_alltime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b781567",
   "metadata": {},
   "source": [
    "# Accessing scraped raw data in AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e394a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"aws_credential.txt\", 'r') as file:\n",
    "    aws_credential=json.load(file)\n",
    "s3=boto3.Session(\n",
    "    profile_name = None, \n",
    "    region_name = 'us-east-2').client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_credential['access_key'],\n",
    "    aws_secret_access_key=aws_credential['secret_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547bcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_content=s3.list_objects(Bucket = aws_credential['bucket'], Prefix ='visa_output/')['Contents']\n",
    "output_items = [d['Key'].split('/')[-1] for d in output_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84cf1dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'country_code_matches.csv',\n",
       " 'country_list.txt',\n",
       " 'df_iv.csv',\n",
       " 'df_niv.csv',\n",
       " 'iv_alltime.csv',\n",
       " 'niv_alltime.csv',\n",
       " 'time_iv.txt',\n",
       " 'time_niv.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f683eda",
   "metadata": {},
   "source": [
    "**Stop here for now (2025-05-05), use iv_alltime and niv_alltime once new data are scraped**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_niv_content=s3.list_objects(Bucket = aws_credential['bucket'], Prefix ='messy_data/visa_scraped/niv/')['Contents']\n",
    "bucket_iv_content=s3.list_objects(Bucket = aws_credential['bucket'], Prefix ='messy_data/visa_scraped/iv/')['Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_niv_items=[d['Key'].split('/')[-1] for d in bucket_niv_content]\n",
    "bucket_iv_items=[d['Key'].split('/')[-1] for d in bucket_iv_content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_iv_date=[f.split('.')[0].split('_')[-1] for f in bucket_iv_items]\n",
    "bucket_niv_date=[f.split('.')[0].split('_')[-1] for f in bucket_niv_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6dab9",
   "metadata": {},
   "source": [
    "# How many months (raw data) need processing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to use a list of unique time stamps\n",
    "len_diff=len(bucket_niv_items) - len(old_data.time.unique())\n",
    "if len_diff > 0:\n",
    "    print(f'Need to append {len_diff} month(s) to existing NIV data.')\n",
    "else: print(\"No actions required, compiled NIV data is up to date.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411edefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to use a list of unique time stamps\n",
    "len_diff=len(bucket_iv_items) - len(old_data.time.unique())\n",
    "if len_diff > 0:\n",
    "    print(f'Need to append {len_diff} month(s) to existing IV data.')\n",
    "else: \n",
    "    print(\"No actions required, compiled IV data is up to date.\")\n",
    "    sys.exit(0) # exit(0) with grace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.time.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(bucket_niv_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f96cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last item minus the difference in length\n",
    "bucket_niv_date[-len_diff-1] == old_data.time.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "niv_date_to_add=bucket_niv_date[-len_diff:]\n",
    "print(f'Needs the following month(s): {niv_date_to_add}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efa3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_niv_date[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_date_to_add = bucket_iv_date[-len_diff:]\n",
    "print(f'Needs the following month(s): {iv_date_to_add} for IV.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c1cba",
   "metadata": {},
   "source": [
    "# Read in raw data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de40b2",
   "metadata": {},
   "source": [
    "## Non-immigrant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "niv_df_raw = [None] * len_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d91074",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_niv_items[-(0+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2618fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len_diff):\n",
    "    print(f'Getting the {-(i+1)} item from the .txt folder:')\n",
    "    file = s3.get_object(Bucket = aws_credential['bucket'],\n",
    "              # offset zero indexing: -(i+1), start from most recent (last in)\n",
    "              Key = 'messy_data/visa_scraped/niv/'+bucket_niv_items[-(i+1)])['Body'].read()\n",
    "    #niv_df_raw[i] = pd.DataFrame(file.decode(\"utf-8\").split('\\n')) # this works\n",
    "    niv_df_raw[i] = pd.read_csv(io.BytesIO(file), delimiter = \"\\t\", header = None) # also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_object=s3.get_object(Bucket = aws_credential['bucket'], \n",
    "              Key = 'messy_data/visa_scraped/niv/' + bucket_niv_items[-len_diff])['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64001a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_object.decode(\"utf-8\").split('\\n')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df003be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_object.decode(\"utf-8\").split('\\n')).iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(io.BytesIO(test_object), delimiter = \"\\t\", header = None).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd925509",
   "metadata": {},
   "source": [
    "## Immigrant visa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea74460",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_df_raw = [None] * len_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043dfb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len_diff):\n",
    "    print(f'Getting the {-(i+1)} item from the .txt folder:')\n",
    "    file = s3.get_object(Bucket = aws_credential['bucket'],\n",
    "              # offset zero indexing: -(i+1), start from most recent (last in)\n",
    "              Key = 'messy_data/visa_scraped/iv/'+bucket_iv_items[-(i+1)])['Body'].read()\n",
    "    #niv_df_raw[i] = pd.DataFrame(file.decode(\"utf-8\").split('\\n')) # this works\n",
    "    iv_df_raw[i] = pd.read_csv(io.BytesIO(file), delimiter = \"\\t\", header = None) # also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(iv_df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f771c",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63ab64",
   "metadata": {},
   "source": [
    "## A list of raw dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_total = []\n",
    "for i in range(len(niv_df_raw)):\n",
    "    niv_df_raw[i].columns = ['V'] # more robust than .rename\n",
    "    # or use .apply(lambda x: x.strip()), but the following is simple\n",
    "    niv_df_raw[i]['V']=niv_df_raw[i]['V'].str.strip().str.upper()\n",
    "    # insert an iterrows() loop to get the index of the grand total row\n",
    "    for idx,row in niv_df_raw[i].iterrows():\n",
    "        if 'grand total'.upper() in row['V']:\n",
    "            grand_total.append(row)\n",
    "            idx_rm_below = idx\n",
    "    niv_df_raw[i]=niv_df_raw[i].iloc[:idx_rm_below]\n",
    "    # offset zero indexing: -(i+1), start from most recent\n",
    "    niv_df_raw[i]['time'] = niv_date_to_add[-(i+1)]\n",
    "print(grand_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_grand_total = []\n",
    "for i in range(len(iv_df_raw)):\n",
    "    iv_df_raw[i].columns = ['V']\n",
    "    iv_df_raw[i]['V']=iv_df_raw[i]['V'].str.strip().str.upper()\n",
    "    for idx,row in iv_df_raw[i].iterrows():\n",
    "        if 'grand total'.upper() in row['V']:\n",
    "            iv_grand_total.append(row)\n",
    "            idx_rm_below = idx\n",
    "    iv_df_raw[i] = iv_df_raw[i].iloc[:idx_rm_below]\n",
    "    iv_df_raw[i]['time'] = iv_date_to_add[-(i+1)]\n",
    "print(iv_grand_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5f5b6",
   "metadata": {},
   "source": [
    "## Concatenated one long dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_niv=pd.concat([df for df in niv_df_raw])\n",
    "df_niv = df_raw_niv.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e57404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_iv=pd.concat([df for df in iv_df_raw])\n",
    "df_iv=df_raw_iv.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067e963",
   "metadata": {},
   "source": [
    "## Remove non-data rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa8677",
   "metadata": {},
   "source": [
    "### NIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1541072",
   "metadata": {},
   "outputs": [],
   "source": [
    "niv_headers = ['NONIMMIGRANT','NATIONALITY VISA','\\\\(FY', '\\\\#SBU','PAGE','SENSITIVE']\n",
    "'|'.join([h for h in niv_headers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_niv[df_niv['V'].str.len() <=1]\n",
    "df_niv_headers=df_niv[df_niv['V'].str.contains('|'.join(niv_headers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niv_headers.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b27ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that were headers & footers, not final yet\n",
    "df_niv=df_niv.iloc[~df_niv.index.isin(df_niv_headers.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e865b2",
   "metadata": {},
   "source": [
    "### IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv['V'] = df_iv['V'].str.strip()\n",
    "df_iv = df_iv[df_iv['V'].str.len() > 1]\n",
    "iv_headers = ['PAGE ', 'FOREIGN STATE OF', 'CHARGEABILITY', \n",
    "              'PLACE OF BIRTH', '\\\\(FY 20', '\\\\(FY20',\n",
    "              'IMMIGRANT VISA', 'SENSITIVE']\n",
    "df_iv_headers = df_iv.loc[df_iv['V'].str.contains('|'.join(iv_headers))]\n",
    "df_iv = df_iv.iloc[~df_iv.index.isin(df_iv_headers.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2638bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv_headers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cff598",
   "metadata": {},
   "source": [
    "## Split all-in-one column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c72bc",
   "metadata": {},
   "source": [
    "### NIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ffa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the warning messages\n",
    "pd.options.mode.copy_on_write = True\n",
    "df_niv['nationality']=[' '.join(row.split(' ')[:-2]).strip() for row in df_niv['V']]\n",
    "# visa class\n",
    "df_niv['visa']=[row.split(' ')[-2].strip() for row in df_niv['V']]\n",
    "# remove thousand separator , from numbers\n",
    "df_niv['issue']=[row.split(' ')[-1].replace(',','').strip() for row in df_niv['V']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3596003",
   "metadata": {},
   "source": [
    "### IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d23b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv['nationality']=[' '.join(row.split(' ')[:-2]).strip() for row in df_iv['V']]\n",
    "df_iv['visa']=[row.split(' ')[-2].strip() for row in df_iv['V']]\n",
    "df_iv['issue']=[row.split(' ')[-1].replace(',','').strip() for row in df_iv['V']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53445ebb",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fee94b",
   "metadata": {},
   "source": [
    "## Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test list comprehension with a short and simple list\n",
    "[s for s in ['34',23,'a1','20','b '] if not str(s).isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d38179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there non-numeric values in the visa issuance count column?\n",
    "check_numeric=[s for s in df_niv['issue'] if not str(s).isdigit()]\n",
    "if len(check_numeric)>0:\n",
    "    print(\"At least one row has non-numeric values in the NIV issuance column. Go back and check.\")\n",
    "    print(check_numeric)\n",
    "    sys.exit()\n",
    "else:\n",
    "    print(\"No non-numeric values were detected in the NIV issuance column. Good to proceed.\")\n",
    "    df_niv['issue'] = df_niv['issue'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there non-numeric values in the visa issuance count column?\n",
    "check_numeric=[s for s in df_iv['issue'] if not str(s).isdigit()]\n",
    "if len(check_numeric)>0:\n",
    "    print(\"At least one row has non-numeric values in the IV issuance column. Go back and check.\")\n",
    "    print(check_numeric)\n",
    "    sys.exit()\n",
    "else:\n",
    "    print(\"No non-numeric values were detected in the IV issuance column. Good to proceed.\")\n",
    "    df_iv['issue'] = df_iv['issue'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2bb20",
   "metadata": {},
   "source": [
    "## Restoring rows when data got mixed with headers\n",
    "\n",
    "- This is the most challenging part of cleaning this dataset.\n",
    "- `if any(pattern in input_text for pattern in pattern_list):`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efcc15",
   "metadata": {},
   "source": [
    "### NIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b621e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_idx = []\n",
    "# here's how any() works\n",
    "for idx,row in df_niv_headers.iterrows():\n",
    "    if any(c in row['V'] for c in df_niv.nationality.unique()):\n",
    "        print(idx, row)\n",
    "        restore_idx.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restore=df_niv_headers.loc[restore_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ff794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restore['nationality']=[' '.join(row.split('NONIMMIGRANT')[0].strip().split(' ')[:-2]).strip() for row in df_restore['V']]\n",
    "df_restore['visa'] = [row.split('NONIMMIGRANT')[0].strip().split(' ')[-2].strip() for row in df_restore['V']]\n",
    "df_restore['issue'] = [row.split('NONIMMIGRANT')[0].strip().split(' ')[-1].strip().replace(',','') for row in df_restore['V']]\n",
    "df_restore['issue'] = df_restore['issue'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80d2ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_restore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373f928",
   "metadata": {},
   "source": [
    "### IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_idx_iv = []\n",
    "# here's how any() works\n",
    "for idx,row in df_iv_headers.iterrows():\n",
    "    if any(c in row['V'] for c in df_iv.nationality.unique()):\n",
    "        print(idx, row)\n",
    "        restore_idx_iv.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restore_iv=df_iv_headers.loc[restore_idx_iv]\n",
    "df_restore_iv['nationality']=[' '.join(row.split('IMMIGRANT')[0].strip().split(' ')[:-2]).strip() for row in df_restore_iv['V']]\n",
    "df_restore_iv['visa'] = [row.split('IMMIGRANT')[0].strip().split(' ')[-2].strip() for row in df_restore_iv['V']]\n",
    "df_restore_iv['issue'] = [row.split('IMMIGRANT')[0].strip().split(' ')[-1].strip().replace(',','') for row in df_restore_iv['V']]\n",
    "df_restore_iv['issue'] = df_restore_iv['issue'].astype(int)\n",
    "df_restore_iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d47d0c",
   "metadata": {},
   "source": [
    "## Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['nationality', 'visa', 'issue','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niv=pd.concat([df_niv, df_restore]).sort_index().drop(columns = ['V'])[col_order].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98760228",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grand_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niv['issue'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv=pd.concat([df_iv, df_restore_iv]).sort_index().drop(columns=['V'])[col_order].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b174427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv['issue'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niv = df_niv.rename(columns={'issue':'count'})\n",
    "df_iv = df_iv.rename(columns={'issue':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb533a71",
   "metadata": {},
   "source": [
    "# Appending to existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92935a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "niv_alltime=s3.get_object(Bucket = aws_credential['bucket'], \n",
    "              Key = 'visa_output/niv_alltime.csv')['Body'].read()\n",
    "df_niv_alltime = pd.read_csv(io.BytesIO(niv_alltime),low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_alltime=s3.get_object(Bucket = aws_credential['bucket'], \n",
    "              Key = 'visa_output/iv_alltime.csv')['Body'].read()\n",
    "df_iv_alltime = pd.read_csv(io.BytesIO(iv_alltime),low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niv_alltime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv_alltime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefcf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niv_alltime_new=pd.concat([df_niv_alltime, df_niv]).reset_index(drop=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7289ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv_alltime_new=pd.concat([df_iv_alltime, df_iv]).reset_index(drop=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efab8df",
   "metadata": {},
   "source": [
    "# Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee653f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list=list(set(list(df_iv_alltime_new['nationality'].unique()) + \n",
    "                      list(df_niv_alltime_new['nationality'].unique()\n",
    "                          )\n",
    "                     )\n",
    "                 )\n",
    "print(\"Total unique country/nationality labels before cleaning: \",len(country_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = []\n",
    "# instead of iterrows, can also work on a list of unique nationalities\n",
    "for country in country_list:\n",
    "    for char in country:\n",
    "        if not (char.isalpha() or char == ' '):\n",
    "            if char not in special_chars:\n",
    "                special_chars.append(char)\n",
    "                print(char, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars.remove(',') # potential legit\n",
    "special_chars.remove(\"'\") # potential legit\n",
    "special_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_country_label = []\n",
    "new_country_label = []\n",
    "for country in country_list:\n",
    "    # the following covers the case when one string contains multiple special characters: e.g., '(' and ')'\n",
    "    if any(char in country for char in special_chars):\n",
    "        # if it's a letter or a space, join as usual, then replace special character with a space\n",
    "        new_country = ''.join([char if (char.isalpha() or char == ' ') \n",
    "                               else char.replace(char,' ') \n",
    "                               for char in country]) # replace special character with space\n",
    "        new_country = ' '.join(new_country.split()).replace('BORN','').strip() # split() to remove excessive space\n",
    "        old_country_label.append(country)\n",
    "        new_country_label.append(new_country)\n",
    "        print(\"\\nold: \",country,'\\nnew: ', new_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65231b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sp_char_label=dict(zip(old_country_label,new_country_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map new country labels to a new column nationality2, then replace it with original nationality\n",
    "df_niv_alltime_new['nationality2'] = df_niv_alltime_new['nationality'].map(\n",
    "    no_sp_char_label).fillna(\n",
    "    df_niv_alltime_new['nationality'])\n",
    "df_iv_alltime_new['nationality2'] = df_iv_alltime_new['nationality'].map(\n",
    "    no_sp_char_label).fillna(\n",
    "    df_iv_alltime_new['nationality'])\n",
    "df_niv_alltime_new = df_niv_alltime_new.drop(\n",
    "    columns=['nationality']).rename(\n",
    "    columns={'nationality2':'nationality'})\n",
    "df_iv_alltime_new = df_iv_alltime_new.drop(\n",
    "    columns=['nationality']).rename(\n",
    "    columns={'nationality2':'nationality'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list_new=list(set(list(df_iv_alltime_new['nationality'].unique()) + \n",
    "                      list(df_niv_alltime_new['nationality'].unique()\n",
    "                          )\n",
    "                     )\n",
    "                 )\n",
    "print(\"Total unique country/nationality labels after cleaning: \",len(country_list_new))\n",
    "print(f'After removing special characters, {len(country_list) - len(country_list_new)} labels were reduced.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb3feb",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3caad2",
   "metadata": {},
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f763ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('country_list.txt','w') as file:\n",
    "    file.write('\\n'.join(country_list_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['nationality','visa', 'count', 'time']\n",
    "df_niv_alltime_new = df_niv_alltime_new[new_col_order]\n",
    "df_iv_alltime_new = df_iv_alltime_new[new_col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c10dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_niv=list(df_niv_alltime_new['time'].unique())\n",
    "time_iv=list(df_iv_alltime_new['time'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output a list of time stamps\n",
    "with open('time_niv.txt', 'w') as file:\n",
    "    file.write('\\n'.join(time_niv))\n",
    "with open('time_iv.txt', 'w') as file:\n",
    "    file.write('\\n'.join(time_iv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d6568",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list a string to upload\n",
    "s3.put_object(Body = \"\\n\".join([c for c in country_list_new]), \n",
    "              Bucket = aws_credential['bucket'], \n",
    "              Key = 'visa_output/country_list.txt')\n",
    "s3.put_object(Body = \"\\n\".join([t for t in time_niv]), \n",
    "              Bucket = aws_credential['bucket'], \n",
    "              Key = 'visa_output/time_niv.txt')\n",
    "s3.put_object(Body = \"\\n\".join([t for t in time_iv]), \n",
    "              Bucket = aws_credential['bucket'], \n",
    "              Key = 'visa_output/time_iv.txt')\n",
    "# list objects after upload\n",
    "output_folder_items=s3.list_objects(Bucket = aws_credential['bucket'], Prefix = 'visa_output')['Contents']\n",
    "item_names=[d['Key'] for d in output_folder_items]\n",
    "[item for item in item_names if item.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffec44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload pandas dataframe to s3\n",
    "csv_buffer = io.StringIO()\n",
    "df_niv_alltime_new.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Body = csv_buffer.getvalue(), \n",
    "              Bucket = aws_credential['bucket'], \n",
    "              Key = 'visa_output/df_niv.csv')\n",
    "# create a new csv buffer object to upload a different data frame\n",
    "csv_buffer = io.StringIO()\n",
    "df_iv_alltime_new.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Body = csv_buffer.getvalue(), \n",
    "              Bucket = aws_credential['bucket'], \n",
    "              Key = 'visa_output/df_iv.csv')\n",
    "# list objects after upload\n",
    "output_folder_items=s3.list_objects(Bucket = aws_credential['bucket'], Prefix = 'visa_output')['Contents']\n",
    "item_names=[d['Key'] for d in output_folder_items]\n",
    "[item for item in item_names if item.endswith('.csv')]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py-3-12-3-preview",
   "language": "python",
   "name": "python-312-preview"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
